---
---

@misc{Agrawal_Bania_Bhisikar_Bisoi_Mestha_S_2024, 
      title={Grasping Graphormerâ€¯: Assessing Transformer Performance for Graph Representation}, 
      url={https://gram-blogposts.github.io/blog/2024/graphormer/},
      html={https://gram-blogposts.github.io/blog/2024/graphormer/},
      abstract={A first-principles blog post to understand the Graphormer.},
      author={Agrawal, Tejas and Bania, Karan and Bhisikar, Yash and Bisoi, Ankita and Mestha, Harshvardhan and S, Sarang}, 
      booktitle={ICML 2024 Workshop on Geometry-grounded Representation Learning and Generative Modeling},
      abb={ICML},
      preview={graphormer_img.png},
      year={2024}, month={Jun}
} 

@misc{mestha2024countclipreteaching,
      title={CountCLIP -- [Re] Teaching CLIP to Count to Ten}, 
      abstract={Large vision-language models (VLMs) are shown to learn rich joint image-text representations enabling high performances in relevant downstream tasks. However, they fail to showcase their quantitative understanding of objects, and they lack good counting-aware representation. This paper conducts a reproducibility study of 'Teaching CLIP to Count to Ten' (Paiss et al., 2023), which presents a method to finetune a CLIP model (Radford et al., 2021) to improve zero-shot counting accuracy in an image while maintaining the performance for zero-shot classification by introducing a counting-contrastive loss term. We improve the model's performance on a smaller subset of their training data with lower computational resources. We verify these claims by reproducing their study with our own code. The implementation can be found at [this URL](https://github.com/SforAiDl/CountCLIP).},
      html={https://arxiv.org/abs/2406.03586},
      code={https://github.com/SforAiDl/CountCLIP},
      author={Harshvardhan Mestha and Tejas Agrawal and Karan Bania and Shreyas V and Yash Bhisikar},
      year={2024},
      eprint={2406.03586},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      preview={countclip_img.gif},
      url={https://arxiv.org/abs/2406.03586}, 
}
